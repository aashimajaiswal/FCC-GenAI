{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv  import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/SDGs_Booklet_Web_En.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting documents: store all the text in one place\n",
    "question_gen = \"\"\n",
    "for page in data:\n",
    "    question_gen += page.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "question_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "# Recommended: Can input strings but better to convert ur string to document\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter_ans_gen = TokenTextSplitter(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_ques_gen = splitter_ans_gen.split_text(question_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ques_gen = [Document(page_content=chunk) for chunk in chunk_ques_gen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter_ans_gen = TokenTextSplitter(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ans_gen = splitter_ans_gen.split_documents(document_ques_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "document_ques_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_ques_gen_pipeline = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"You are an expert instructor in {topic}, preparing a student for an upcoming exam based\" \\\n",
    "\" on {source_material}. Generate 5 critical exam-style questions covering the most essential concepts and topics \" \\\n",
    "\"from the provided material. Ensure the questions effectively reflect what the student must know to excel\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_QUESTIONS = PromptTemplate(template=prompt_template, input_fields=[\"topic\", \"source_material\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm_ques_gen_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMChain\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create the chain\u001b[39;00m\n\u001b[1;32m      4\u001b[0m question_gen_chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[0;32m----> 5\u001b[0m     llm\u001b[38;5;241m=\u001b[39m\u001b[43mllm_ques_gen_pipeline\u001b[49m,\n\u001b[1;32m      6\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mPROMPT_QUESTIONS\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     10\u001b[0m topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSustainable Development Goals\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm_ques_gen_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Create the chain\n",
    "question_gen_chain = LLMChain(\n",
    "    llm=llm_ques_gen_pipeline,\n",
    "    prompt=PROMPT_QUESTIONS\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "topic = \"Sustainable Development Goals\"\n",
    "source_material = question_gen\n",
    "generated_questions = question_gen_chain.run(topic=topic, source_material=source_material)\n",
    "print(generated_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_list = generated_questions.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_list = [q for q in ques_list if q.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "ques_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings \n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectordb\n",
    "from langchain.vectorstores import FAISS\n",
    "vector_store = FAISS.from_documents(document_ans_gen,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_ans_gen_pipeline = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "gen_chain = RetrievalQA.from_chain_type(llm = llm_ans_gen_pipeline, chain_type = \"stuff\",\n",
    "                                         retriever = vector_store.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# ask the questions\n",
    "for q in ques_list:\n",
    "    answer = gen_chain.run(q)\n",
    "    print(f\"Question: {q}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"\\n\")\n",
    "    with open(\"answers.txt\", \"a\") as f:\n",
    "        f.write(f\"Question: {q}\\n\")\n",
    "        f.write(f\"Answer: {answer}\\n\")\n",
    "        f.write(\"---------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
